# 🗓️ 2025.07.31(목) – 교육 일지

## 📚 오늘 배운 내용
### LLM
#### OpenAI API의 Chat Completions을 이용하여 챗봇 만들기
- 환경 변수 및 OPEN_API_KEY 설정
  ```
  # OPEN_API_KEY 설정
  import os
  from dotenv import load_dotenv
  
  load_dotenv() # 현재 경로의 .env 파일을 읽어 시스템 환경 변수로 등록
  
  OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
  ```
- 예시
  ```
  from openai import OpenAI
  client = OpenAI(api_key=OPENAI_API_KEY)
  
  response = client.chat.completions.create(
  model="gpt-4.1",
  messages=[
  {"role": "system", "content": "모델의 성격과 역할을 지정"},
  {"role": "user", "content": "사용자의 요청"}
  ],
  temperature=1.0,
  max_tokens=2048
  )
  ```
  | 매개변수                   | 설명                                                  |
  | ---------------------- | --------------------------------------------------- |
  | **model**              | 사용할 모델 (예: `gpt-4.1`, `gpt-4o`)                     |
  | **messages**           | 대화 히스토리. `role`은 `system`, `user`, `assistant` 중 하나 |
  | **temperature**        | 창의성 정도 (0 \~ 2, 높을수록 창의적)                           |
  | **max\_tokens**        | 응답 토큰 수 제한                                          |
  | **top\_p**             | 확률 기반 샘플링 범위 (1이면 모든 토큰 고려)                         |
  | **frequency\_penalty** | 동일 단어 반복 감소                                         |
  | **presence\_penalty**  | 새로운 주제 언급 촉진                                        |
  - system
    - 모델의 "성격"과 "행동 지침"을 가장 먼저 설정
    - 길고 구체적일수록 모델의 일관성 ↑
  - user
    - 실제 질의나 입력 데이터 전달
  - 기타 파라미터
    - temperature ↑ → 창의성 높음, 일관성 낮음
    - max_tokens → 응답 길이 제한
    - frequency_penalty, presence_penalty로 문장 반복 조절 가능
    
#### Few-Shot Learning
- 정의 : Few-shot은 모델에게 예시를 몇 개 보여주고 그 패턴을 따라 하게 하는 방법
- 예시
  ```
  system_instruction = """
  ### 예시 ###
  - 원래 제목 : "어제 서울에서 큰불이 나서 수백명이 대피했다."
  - 교정 제목 : "서울 대형화재, 수백명 대피"
  """
  ```
- 특징
  - 예시가 많을수록(=many-shot) 모델이 더 잘 따라하지만, 토큰 소모 ↑
  - 예시를 하나만 주면 one-shot
  - 예시 없이 규칙만 주면 zero-shot

#### 생각의 사슬
- 정의 : 모델이 답을 내기 전에 중간 사고 과정을 단계별로 작성하도록 유도하는 프롬프트 기법
- CoT 장점
  - 복잡한 문제에서 정답률 ↑ (특히 수학, 논리, 계획 세우기)
  - 출력이 구조화되어 가독성 ↑
  - 사고 경로를 명시하니 디버깅 쉬움
- CoT 작성 팁
  - "단계별로 생각해봐" / "Step-by-step" / "Reason → Plan → Act" 같은 표현 포함
  - 번호 매기기, 소제목 사용
  - 중간 과정도 출력하도록 요구 (숨기고 싶으면 internal reasoning 기법 사용)


#### 스트리밍 응답
- 기능
  - 모델이 응답을 다 만든 후에 한 번에 보내는 게 아니라, 단어·문장 단위로 조각(chunk)들을 실시간으로 전송
  - 대기 시간이 길어지는 긴 답변일 때, 중간에 이미 생성된 부분을 바로 화면에 보여줄 수 있음
- 예시
  ```
  stream_response = client.chat.completions.create(
    model="gpt-4o",
    messages=[ ... ],
    stream=True
  )
  ```
- 핵심 동작
  ```
  for chunk in stream_response:
    if chunk.choices[0].delta.content is not None:
      print(chunk.choices[0].delta.content, end="")
  ```
  - `stream=True` → API 응답이 제너레이터처럼 동작.
  - `for chunk in stream_response:` → 모델이 보낸 조각을 순차적으로 받음.
  - `chunk.choices[0].delta.content` → 조각 안의 텍스트 부분.
  - `end=""` → 줄바꿈 없이 이어 붙이기.

#### 대화 히스토리 기반 요청
- 기능
  - 대화 맥락을 유지
  - message 배열이 지금까지의 대화를 전부 담고 있다.
  - 모델을 이전 `assistant`·`user` 발화를 참고해 문맥 있는 답변을 생성.
- 예시
  ```
  response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {
            "role" : "system",
            "content" : "당신은 이성적이고 상세한 설명을 잘 하는 챗봇입니다."
        },
        {
            "role" : "user",
            "content" : "안녕하세요, 제 이름은 김시방방구리구리입니다."
        },
        {
            "role" : "assistant",
            "content" : "안녕하세요, 김시방방구리구리님! 만나서 반갑습니다. 어떻게 도와드릴까요?"
        },
        {
            "role" : "user",
            "content" : "내 이름이 뭐라고???"
        }
    ],
  )
  ```
- role별 의미
  - "system" → 모델의 성격·규칙 설정
  - "user" → 사용자가 한 말
  - "assistant" → 모델이 과거에 대답했던 내용
  - 마지막 "user" → 이번 요청
- 사용 장점
  - 다중 턴 대화(Multi-turn conversation)에서 이전 내용을 기억하고 이어서 대화 가능
  - 챗봇, 고객센터, 인터뷰 시뮬레이터 같은 앱에 필수
  
## ✍️ 오늘의 회고
- 오늘은 다양한 프롬프트 기법에 대해서 배웠다. 단순히 질문을 던지는 것보다 프롬프트 설계가 답변의 질을 크게 좌우한다는 것을 꺠달았다.
- 스트리밍 방식은 기술적으로 어렵지 않으면서도 체감 UX 개선이 커서 실제 서비스에 꼭 적용할 예정이다.